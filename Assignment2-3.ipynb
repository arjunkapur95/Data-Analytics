{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# INFO3406 - Introduction to Data Analytics\n",
    "## Assignment 2 -  Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Instructions **\n",
    "\n",
    "#### Kaggle Display Advertising Challenge dataset will be used in this assignment. It contains of 39 features of online ads and information of if each ad was clicked or not over a period of 7 days. The semantic of these features is undisclosed. The overall objective is to determine if an ad will be clicked or not, for a set of query of features.\n",
    "\n",
    "#### For this assignment, only 100,000 ads will be used. The dataset should be downloaded according to the instructions in part 1). The first column of *\"dac_sample.txt\"*  indicates if an ad was clicked (=1) or not (=0) while rest of the 39 columns contain feature values. For this assignment you can consider all features are categorical. The values of some of these features have been hashed for anonymization purposes. Note that some features have missing values.\n",
    "\n",
    "\n",
    "\n",
    "You may view spark stages from\n",
    "http://localhost:4040/stages/ \n",
    "\n",
    "\n",
    "Some other useful resources/references:\n",
    "+ [Map-Reduce for Machine Learning on Multicore](http://papers.nips.cc/paper/3150-map-reduce-for-machine-learning-on-multicore.pdf)\n",
    "+ [Spark RDD](http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf)\n",
    "+ [Display Advertising Challenge](https://www.kaggle.com/c/criteo-display-ad-challenge)\n",
    "+ [MLlib](https://spark.apache.org/docs/1.1.0/mllib-guide.html)\n",
    "+ [MLlib: Scalable Machine Learning on Spark](http://stanford.edu/~rezab/sparkworkshop/slides/xiangrui.pdf)\n",
    "+ [Scalable Machine Learning](https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Preparing the dataset **\n",
    "\n",
    "#### Import *\"assignment2LoadData.py\"* file to the [*jupyter*  home](http://localhost:8001/tree) folder.  Go to [http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/](http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/) to download the dataset. After you accept the agreement, you can obtain the dataset by clicking on the ***\"Download Sample\"*** button.  The file is 8.4 MB compressed (.tar.gz). Import the compressed data file to the  [*jupyter*  home](http://localhost:8001/tree) folder. The script in *\"assignment2LoadData.py\"* will automatically extract the file to the virtual machine (VM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName is data/Assignment2/dac_sample.txt\n",
      "File is already available. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "import assignment2LoadData as ld\n",
    "\n",
    "ld.extractData(\"data/Assignment2/dac_sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of rawData entry:  [u'0,1,1,5,0,1382,4,15,2,181,1,2,a,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,a,3a171ecb,c5c50484,e8b83407,9727dd16']\n",
      "0,1,1,5,0,1382,4,15,2,181,1,2,a,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,a,3a171ecb,c5c50484,e8b83407,9727dd16\n",
      "\n",
      "rawData count= 100000\n"
     ]
    }
   ],
   "source": [
    "#Extract\n",
    "import numpy as np\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "from __future__ import division\n",
    "t0 = time.time()\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('Assignment2', 'dac_sample.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "partitions = 1\n",
    "if os.path.isfile(fileName):\n",
    "    rawData_0 = (sc\n",
    "               .textFile(fileName, partitions)\n",
    "               .map(lambda x: x.replace('\\t', ','))\n",
    "              )\n",
    "    # work with either ',' or '\\t' separated data\n",
    "    rawData_1 = rawData_0.map(lambda x: x.replace(',,', ',a,'))\n",
    "    rawData = rawData_1.map(lambda x: x.replace(',,', ',a,'))  \n",
    "    print 'An example of rawData entry: ', rawData.take(1)\n",
    "    print rawData.collect()[0]\n",
    "    nData = rawData.count()\n",
    "    print '\\nrawData count=', nData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Decrease or increase sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decrease the sample size for quick testing and debugging purposes\n",
    "\n",
    "# n values to take from rawData\n",
    "n = 20000\n",
    "\n",
    "# takes n values from rawData and converts it to list\n",
    "rawData = rawData.take(n)\n",
    "\n",
    "#convert the list back to RDD\n",
    "rawData = sc.parallelize(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rawTrainData count= 18035\n",
      "rawTestData count= 1965\n",
      "0,2,0,44,1,102,8,2,2,4,1,1,a,4,68fd1e64,f0cf0024,6f67f7e5,41274cd7,25c83c98,fe6b92e5,922afcc0,0b153874,a73ee510,2b53e5fb,4f1b46f3,623049e6,d7020589,b28479f6,e6c5b5cd,c92f3b61,07c540c4,b04e4670,21ddcdc9,5840adea,60f6221e,a,3a171ecb,43f13e8b,e8b83407,731c3655\n"
     ]
    }
   ],
   "source": [
    "weights = [.9, .1]\n",
    "seed = 10\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawTestData = rawData.randomSplit(weights, seed)\n",
    "# Cache the data\n",
    "#rawTrainData.cache()\n",
    "#rawTestData.cache()\n",
    "\n",
    "nTrain = rawTrainData.count()\n",
    "nTest = rawTestData.count()\n",
    "\n",
    "print 'rawTrainData count=', nTrain\n",
    "print 'rawTestData count=', nTest\n",
    "print rawTestData.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of OHE features = 84859\n"
     ]
    }
   ],
   "source": [
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples. \n",
    "    Args: \n",
    "        point (str): A comma separated string where the first value is the label and the rest are features. \n",
    "    Returns: \n",
    "        list: A list of (featureID, value) tuples. \"value\" is a category from 0 to 38. \n",
    "    \"\"\"\n",
    "    items = point.split(',')\n",
    "    return [(i, item) for i, item in enumerate(items[1:])]\n",
    "\n",
    "parsedTrainData = rawTrainData.map(parsePoint)\n",
    "parsedTestData = rawTestData.map(parsePoint)\n",
    "parsedData = rawData.map(parsePoint)\n",
    "\n",
    "#number of different things for each feature\n",
    "numCategories = (parsedData\n",
    "                 .flatMap(lambda x: x)\n",
    "                 .distinct()\n",
    "                 .map(lambda x: (x[0], 1))\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "                 .sortByKey()\n",
    "                 .collect())\n",
    "\n",
    "def createOneHotDict(inputData):\n",
    "    \"\"\"\n",
    "    Creates a one-hot-encoder dictionary based on the input data.\n",
    "    \n",
    "    Args: \n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is made up of a list \n",
    "        of (featureID, value) tuples.\n",
    "    Returns: \n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are unique integers. \n",
    "    \"\"\"\n",
    "    distinctFeats = inputData.flatMap(lambda x: x).distinct() #Get the flatmap of inputData, then distinct() \n",
    "    return distinctFeats.zipWithIndex().collectAsMap()\n",
    "\n",
    "#Create one hot dictionary from parsedTrainFeat \n",
    "ctrOHEDict = createOneHotDict(parsedData) \n",
    "\n",
    "numOHEFeats = len(ctrOHEDict.keys()) \n",
    "print '\\nNumber of OHE features =', numOHEFeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Hashing\n",
    "\n",
    "Reference: http://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Min Hashing\n",
    "\n",
    "# a and b are random coefficients\n",
    "# c is a random prime number greater than numOHEFeats\n",
    "numHashes = 10\n",
    "\n",
    "a = random.sample(range(0, numOHEFeats), 10)\n",
    "b = random.sample(range(0, numOHEFeats), 10)\n",
    "\n",
    "# c = 273551 # c for whole data set\n",
    "c = 84859 # c for 20000 sample\n",
    "\n",
    "# hashes every unique id obtained from OHEDict\n",
    "\n",
    "def hashFunction(ids):\n",
    "    hashes=[]\n",
    "    for i in range(0, numHashes):\n",
    "        hashID = list( map(lambda x: \n",
    "                           (a[i] * x + b[i]) %c ,ids))\n",
    "        minHash = reduce(lambda x,y:\n",
    "                         x if x<y else y ,hashID)\n",
    "        hashes.append(minHash)\n",
    "    return hashes\n",
    "\n",
    "# get ids for featureID value pairs from OHEDict and returns it\n",
    "def getIDs(ad):\n",
    "    ids = []\n",
    "    for featureValuePair in ad:\n",
    "        featureID = ctrOHEDict[featureValuePair]\n",
    "        ids.append(featureID)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert (featureID, value) pair to IDs\n",
    "trainDataIDs = parsedTrainData.map(getIDs)\n",
    "testDataIDs = parsedTestData.map(getIDs)\n",
    "\n",
    "# hash the IDs using the hashFunction\n",
    "hashedTrainIDs = trainDataIDs.map(hashFunction)\n",
    "hashedTestIDs = testDataIDs.map(hashFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convertToList(RDD):\n",
    "    return RDD.collect()\n",
    "\n",
    "#creat separate buckets function\n",
    "trainIDsList = convertToList(hashedTrainIDs)\n",
    "testIDsList = convertToList(hashedTestIDs)\n",
    "\n",
    "def classifySignatures(hashedIDs):\n",
    "    bucket = []\n",
    "    adNumber = 0\n",
    "    # loop through all the trainData\n",
    "    i = 0\n",
    "    while i != nTrain:\n",
    "        # counter for number of hash matches\n",
    "        counter = 0\n",
    "        j = 0\n",
    "        # loop though all the features in the ads comparing\n",
    "        for hashedID in hashedIDs:\n",
    "            if hashedID == trainIDsList[i][j]:\n",
    "                j += 1\n",
    "                counter += 1\n",
    "        # number of hashes to match        \n",
    "        if counter >= 5 : \n",
    "            bucket.append(adNumber)\n",
    "        # move to next ad\n",
    "        adNumber += 1\n",
    "        i += 1\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classify the hashed ids into buckets\n",
    "buckets = hashedTestIDs.map(classifySignatures)\n",
    "  \n",
    "# convert RDD to list for iteration\n",
    "allBuckets = buckets.collect()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert RDD to list\n",
    "parsedTestDataList = convertToList(parsedTestData)\n",
    "\n",
    "# parses the list for joining\n",
    "def parse(lists):\n",
    "    index = 0\n",
    "    List = []\n",
    "    try:\n",
    "        while index != len(lists):\n",
    "            tmp = [index, lists[index]]\n",
    "            index += 1\n",
    "            List.append(tmp)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    return List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse the list\n",
    "testDataList = parse(parsedTestDataList)\n",
    "bucketsList = parse(allBuckets)\n",
    "\n",
    "#convert back to RDD using spark parallelization technique\n",
    "testDataRDD = sc.parallelize (testDataList)\n",
    "bucketsRDD = sc.parallelize (bucketsList)\n",
    "\n",
    "#join two RDD together\n",
    "combinedList = testDataRDD.join (bucketsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]]\n"
     ]
    }
   ],
   "source": [
    "trainingFeatures = convertToList(parsedTrainData)\n",
    "\n",
    "import operator\n",
    "\n",
    "def calcKNN(row):\n",
    "    k = 5\n",
    "    dis = []\n",
    "    if len(row[1][1]) == 0:\n",
    "        for i in range(0,k):\n",
    "            dis.append([1,0])#for all the neighbours initalizing the distances \n",
    "    else :    \n",
    "        for item in row[1][1]:\n",
    "            count = 0\n",
    "            hammingDistance = 0\n",
    "            for items in trainingFeatures[item]:\n",
    "                if items != row[1][0][count]:#using the hamming distance metric to find the count of difference between 2 signatures\n",
    "                    hammingDistance += 1\n",
    "                count += 1\n",
    "    \n",
    "            dis.append([hammingDistance, item])#adding the hamming distance of the corresponding item\n",
    "    dis.sort(key = operator.itemgetter(1))\n",
    "    return dis\n",
    "\n",
    "knn = combinedList.map(calcKNN)\n",
    "print knn.take(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splits the first column from rawData and returns it\n",
    "# first column = (0,1) whether the ad was clicked or not\n",
    "def  getClickedStatus(dec):\n",
    "    items = dec.split(',')\n",
    "    return [items[0]]\n",
    "\n",
    "# Extract the clicked or not clicked values from the raw data\n",
    "trainClickedStatus = rawTrainData.map(getClickedStatus)\n",
    "testClickedStatus = rawTestData.map(getClickedStatus)\n",
    "\n",
    "# convert RDD type to list\n",
    "trainClickedList = convertToList(trainClickedStatus)\n",
    "testClickedList = convertToList(testClickedStatus)\n",
    "\n",
    "#predicting wether an advertisment was clicked or not by using the weight factors\n",
    "def prediction(values):\n",
    "    x = 0 #clicked weight factor\n",
    "    y = 0 #unclicked weight factor\n",
    "    for i in values:\n",
    "        if trainClickedList[i[1]] == [u'1']:\n",
    "            x=x+(1/(pow(i[0]+1,2)))\n",
    "        else:\n",
    "            y=y+(1/(pow(i[0]+1,2)))\n",
    "    if x >= y:\n",
    "        click=[u'1']\n",
    "    else:\n",
    "        click=[u'0']\n",
    "    return click\n",
    "\n",
    "guess = knn.map(prediction)\n",
    "guessList = convertToList(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.778117048346\n"
     ]
    }
   ],
   "source": [
    "# correct guess counter\n",
    "correctGuesses = 0\n",
    "\n",
    "# compares the guess against actual data\n",
    "for i in range(0,nTest):\n",
    "    if guessList[i] == testClickedList[i]:\n",
    "        correctGuesses += 1\n",
    "\n",
    "print \"Accuracy = \", correctGuesses/nTest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
